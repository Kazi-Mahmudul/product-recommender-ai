# Pipeline Scheduling Configuration
# This file defines various scheduling scenarios and configurations

# Default scheduling configuration
default:
  schedule_interval: "0 2 * * *"  # Daily at 2 AM
  max_active_runs: 1
  catchup: false
  retries: 3
  retry_delay_minutes: 5
  execution_timeout_hours: 2
  email_on_failure: true
  email_on_retry: false

# Environment-specific configurations
environments:
  production:
    schedule_interval: "0 2 * * *"  # Daily at 2 AM
    max_active_runs: 1
    catchup: false
    retries: 5
    retry_delay_minutes: 10
    execution_timeout_hours: 3
    email_on_failure: true
    email_on_retry: false
    email_on_sla_miss: true
    sla_hours: 4
    
  staging:
    schedule_interval: "0 4 * * *"  # Daily at 4 AM
    max_active_runs: 1
    catchup: false
    retries: 3
    retry_delay_minutes: 5
    execution_timeout_hours: 2
    email_on_failure: true
    email_on_retry: false
    
  development:
    schedule_interval: null  # Manual trigger only
    max_active_runs: 2
    catchup: false
    retries: 1
    retry_delay_minutes: 2
    execution_timeout_hours: 1
    email_on_failure: false
    email_on_retry: false

# Business-specific scheduling patterns
business_patterns:
  ecommerce:
    peak_hours:
      schedule_interval: "0 8,12,16,20 * * *"  # Peak shopping hours
      description: "Run during peak e-commerce hours"
      
    inventory_sync:
      schedule_interval: "0 1,13 * * *"  # Twice daily
      description: "Inventory synchronization"
      
    price_updates:
      schedule_interval: "0 */4 * * *"  # Every 4 hours
      description: "Price update synchronization"
      
  financial:
    end_of_day:
      schedule_interval: "0 18 * * 1-5"  # Weekdays at 6 PM
      description: "End of day financial processing"
      
    monthly_reports:
      schedule_interval: "0 2 1 * *"  # First day of month at 2 AM
      description: "Monthly financial reports"
      
  retail:
    store_hours:
      schedule_interval: "0 9-21 * * *"  # Every hour from 9 AM to 9 PM
      description: "During store operating hours"
      
    weekend_batch:
      schedule_interval: "0 6 * * 6,0"  # Weekends at 6 AM
      description: "Weekend batch processing"

# Trigger mechanisms configuration
triggers:
  manual:
    enabled: true
    max_concurrent: 3
    priority_levels: ["low", "normal", "high", "urgent"]
    timeout_minutes: 120
    
  webhook:
    enabled: true
    authentication_required: true
    rate_limit_per_minute: 10
    allowed_sources:
      - "data_provider_api"
      - "monitoring_system"
      - "admin_panel"
      - "external_scheduler"
    
  file_based:
    enabled: true
    watch_directory: "/opt/airflow/triggers"
    file_patterns:
      - "*.trigger"
      - "*.json"
    processing_interval_seconds: 30
    
  api:
    enabled: true
    port: 5000
    authentication_token_env: "PIPELINE_API_TOKEN"
    rate_limit_per_minute: 30
    cors_enabled: true

# Conditional execution rules
conditions:
  data_freshness:
    enabled: true
    staleness_threshold_hours: 24
    check_endpoint: "/api/data/freshness"
    
  system_health:
    enabled: true
    required_services:
      - "scraper"
      - "processor" 
      - "sync"
    health_check_timeout_seconds: 30
    
  maintenance_windows:
    enabled: true
    windows:
      - name: "Daily Maintenance"
        start_time: "03:00"
        end_time: "04:00"
        days: ["sunday"]
        description: "Weekly maintenance window"
        
      - name: "Emergency Maintenance"
        start_time: "01:00"
        end_time: "02:00"
        days: ["monday", "tuesday", "wednesday", "thursday", "friday"]
        description: "Emergency maintenance slot"
        active: false
        
  external_dependencies:
    enabled: true
    dependencies:
      - name: "Source API"
        type: "http"
        url: "https://api.source.com"
        endpoint: "/health"
        timeout_seconds: 10
        
      - name: "Data Files"
        type: "file"
        path: "/data/source/ready.flag"
        max_age_hours: 2

# Adaptive scheduling configuration
adaptive_scheduling:
  enabled: true
  base_interval_minutes: 60
  min_interval_minutes: 30
  max_interval_minutes: 240
  
  performance_thresholds:
    success_rate_min: 0.95
    avg_duration_max_minutes: 45
    error_rate_max: 0.05
    
  scaling_factors:
    scale_up_factor: 0.8    # Decrease interval by 20%
    scale_down_factor: 1.5  # Increase interval by 50%
    
  evaluation_window_hours: 24
  adjustment_cooldown_hours: 4

# Monitoring and alerting for scheduling
monitoring:
  schedule_drift_threshold_minutes: 15
  missed_schedule_alert: true
  long_running_threshold_hours: 4
  
  metrics:
    - "schedule_adherence"
    - "execution_duration"
    - "success_rate"
    - "queue_length"
    - "resource_utilization"
    
  alerts:
    channels:
      - "email"
      - "slack"
      - "webhook"
    
    rules:
      - condition: "missed_schedule"
        severity: "high"
        cooldown_minutes: 30
        
      - condition: "long_running"
        severity: "medium"
        cooldown_minutes: 60
        
      - condition: "high_failure_rate"
        severity: "high"
        cooldown_minutes: 15

# Resource management
resources:
  cpu_limit: "2000m"
  memory_limit: "4Gi"
  disk_limit: "10Gi"
  
  scaling:
    enabled: true
    min_replicas: 1
    max_replicas: 5
    cpu_threshold: 80
    memory_threshold: 85
    
  priority_classes:
    urgent: 1000
    high: 800
    normal: 500
    low: 200

# Backup and recovery scheduling
backup:
  enabled: true
  schedule_interval: "0 1 * * *"  # Daily at 1 AM
  retention_days: 7
  storage_path: "/opt/airflow/backups"
  
  components:
    - "airflow_metadata"
    - "dag_configurations"
    - "execution_logs"
    - "pipeline_variables"

# Security configuration
security:
  authentication:
    required: true
    token_expiry_hours: 24
    
  authorization:
    roles:
      - name: "admin"
        permissions: ["read", "write", "execute", "delete"]
        
      - name: "operator"
        permissions: ["read", "execute"]
        
      - name: "viewer"
        permissions: ["read"]
        
  audit:
    enabled: true
    log_all_actions: true
    retention_days: 90

# Integration settings
integrations:
  prometheus:
    enabled: true
    metrics_port: 9090
    scrape_interval_seconds: 30
    
  grafana:
    enabled: true
    dashboard_refresh_seconds: 30
    
  slack:
    enabled: false
    webhook_url_env: "SLACK_WEBHOOK_URL"
    channel: "#data-pipeline"
    
  email:
    enabled: true
    smtp_host_env: "SMTP_HOST"
    smtp_port_env: "SMTP_PORT"
    from_address_env: "SMTP_FROM"