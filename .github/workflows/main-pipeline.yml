name: Main Data Pipeline

on:
  schedule:
    # Daily at 2 AM Bangladesh time (UTC+6, so 8 PM UTC previous day)
    - cron: '0 20 * * *'
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (leave empty to check all pages with smart updates)'
        required: false
        type: string
        default: ''
      skip_processor_rankings:
        description: 'Skip processor rankings update'
        required: false
        type: boolean
        default: false

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  PIPELINE_ENV: production
  LOG_LEVEL: INFO
  BATCH_SIZE: 50

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      pipeline_run_id: ${{ steps.setup.outputs.pipeline_run_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy==1.24.3
          pip install -r requirements.txt

      - name: Setup pipeline
        id: setup
        run: |
          PIPELINE_RUN_ID="github_$(date +%Y%m%d_%H%M%S)"
          echo "pipeline_run_id=$PIPELINE_RUN_ID" >> $GITHUB_OUTPUT
          echo "ðŸš€ Starting pipeline run: $PIPELINE_RUN_ID"
          echo "ðŸ“… Scheduled time: $(date)"

      - name: Validate environment
        run: |
          python -c "
          import os
          import psycopg2
          
          # Test database connection
          try:
              conn = psycopg2.connect(os.environ['DATABASE_URL'], connect_timeout=10)
              cursor = conn.cursor()
              cursor.execute('SELECT 1')
              result = cursor.fetchone()
              conn.close()
              print('âœ… Database connection: SUCCESS')
          except Exception as e:
              print(f'âŒ Database connection: FAILED - {e}')
              exit(1)
          "

      - name: Run database migrations
        run: |
          python pipeline/database/add_timestamp_columns.py

  extract_data:
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: 120  # 2 hour timeout for extraction
    outputs:
      scraping_result: ${{ steps.extract.outputs.scraping_result }}
      processor_result: ${{ steps.extract.outputs.processor_result }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy==1.24.3
          pip install -r requirements.txt

      - name: Install Chrome for Selenium
        run: |
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Extract data
        id: extract
        run: |
          python pipeline/github_actions/extract_data.py \
            --pipeline-run-id "${{ needs.setup.outputs.pipeline_run_id }}" \
            --max-pages "${{ github.event.inputs.max_pages || '' }}" \
            --skip-processor-rankings "${{ github.event.inputs.skip_processor_rankings || 'false' }}"

  process_and_load_data:
    runs-on: ubuntu-latest
    needs: [setup, extract_data]
    outputs:
      processing_result: ${{ steps.process.outputs.processing_result }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy==1.24.3
          pip install -r requirements.txt

      - name: Process and load data
        id: process
        run: |
          python pipeline/github_actions/process_data.py \
            --pipeline-run-id "${{ needs.setup.outputs.pipeline_run_id }}" \
            --scraping-result '${{ needs.extract_data.outputs.scraping_result }}' \
            --processor-result '${{ needs.extract_data.outputs.processor_result }}'

  validate_data:
    runs-on: ubuntu-latest
    needs: [setup, extract_data, process_and_load_data]
    outputs:
      loading_result: ${{ steps.validate.outputs.loading_result }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy==1.24.3
          pip install -r requirements.txt

      - name: Validate data loading
        id: validate
        run: |
          python pipeline/github_actions/load_data.py \
            --pipeline-run-id "${{ needs.setup.outputs.pipeline_run_id }}" \
            --processing-result '${{ needs.process_and_load_data.outputs.processing_result }}'

  update_top_searched:
    runs-on: ubuntu-latest
    needs: [setup, validate_data]
    outputs:
      top_searched_result: ${{ steps.update.outputs.top_searched_result }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy==1.24.3
          pip install -r requirements.txt

      - name: Update top searched phones
        id: update
        run: |
          python pipeline/github_actions/update_top_searched.py \
            --pipeline-run-id "${{ needs.setup.outputs.pipeline_run_id }}"

  summary:
    runs-on: ubuntu-latest
    needs: [setup, extract_data, process_and_load_data, validate_data, update_top_searched]
    if: always()
    steps:
      - name: Pipeline Summary
        run: |
          echo "## ðŸ“Š Pipeline Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Pipeline Run ID:** ${{ needs.setup.outputs.pipeline_run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Extract data results
          if [ "${{ needs.extract_data.result }}" == "success" ]; then
            echo "âœ… **Data Extraction:** SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Data Extraction:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Process and load data results (combined step)
          if [ "${{ needs.process_and_load_data.result }}" == "success" ]; then
            echo "âœ… **Data Processing & Loading:** SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Data Processing & Loading:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Validation results
          if [ "${{ needs.validate_data.result }}" == "success" ]; then
            echo "âœ… **Data Validation:** SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Data Validation:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Top searched results
          if [ "${{ needs.update_top_searched.result }}" == "success" ]; then
            echo "âœ… **Top Searched Update:** SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Top Searched Update:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Overall Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Architecture:** Direct in-memory database loading (no intermediate CSV files)" >> $GITHUB_STEP_SUMMARY